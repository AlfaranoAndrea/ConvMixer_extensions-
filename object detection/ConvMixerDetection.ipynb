{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29335,
     "status": "ok",
     "timestamp": 1608730945777,
     "user": {
      "displayName": "Sebastián Rodríguez",
      "photoUrl": "",
      "userId": "00297356644844571360"
     },
     "user_tz": 180
    },
    "id": "fBrQd3D-82_8",
    "outputId": "004a238c-74c2-4a12-e60e-a2a96e16b3ab"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.patches as patches\n",
    "import torch.optim as optim\n",
    "\n",
    "import tensorboard \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from  loss import YoloLoss\n",
    "from  utilyties import funcion_ajuste_dataset\n",
    "\n",
    "from PIL import Image, ImageDraw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter= { \"name\": \"ConvMixer\",\n",
    "             \"batch-size\": 1,\n",
    "    \n",
    "             \"hdim\":    768,\n",
    "             \"depth\":   32, #32\n",
    "             \"psize\":   7,\n",
    "             \"conv-ks\": 7,\n",
    "             \"epochs\" : 10,\n",
    "             \"lr-max\": 2e-5,\n",
    "             \"workers\": 32,\n",
    "}\n",
    "#Clases de VOC2007 to id numerico\n",
    "class_to_id = {'aeroplane':0, 'bicycle':1, 'bird':2, 'boat':3, 'bottle':4,'bus':5, 'car':6, 'cat':7, 'chair':8, 'cow':9, 'diningtable':10,\n",
    "                'dog':11, 'horse':12, 'motorbike':13, 'person':14, 'pottedplant':15,'sheep':16, 'sofa':17, 'train':18, 'tvmonitor':19}\n",
    "\n",
    "#Id numerico to Clases de VOC2007\n",
    "id_to_class = {i:c for c, i in class_to_id.items()}\n",
    "\n",
    "S=7\n",
    "C=20\n",
    "B=2\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC(pl.LightningDataModule):\n",
    "    def __init__ (self, batch_size=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size=parameter[\"batch-size\"]   # Defining transforms to be applied on the data\n",
    "        self.reshape_size= 448\n",
    "        self.train_data = datasets.VOCDetection('./', year='2012', image_set='train',\n",
    "                                                 transforms=self.transforms)      \n",
    "        self.test_data = datasets.VOCDetection('./', year='2012', image_set='val',\n",
    "                                                transforms=self.transforms)\n",
    "\n",
    "        self.train_data_clean = datasets.VOCDetection('./', year='2012', image_set='train')\n",
    "        self.test_data_clean = datasets.VOCDetection('./', year='2012', image_set='val')\n",
    "\n",
    "    \n",
    "    def transforms (self,img, ann ):\n",
    "        transormation = transforms.Compose([\n",
    "        transforms.Resize((self.reshape_size, self.reshape_size)), #Se fija el tamaño de las imagenes en 224x224 para evitar variaciones en las input a alas redes\n",
    "        transforms.ToTensor(), #Se transforma a tensor\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "    \n",
    "        box_batch = torch.Tensor(S,S,C+5*B).fill_(0)#Creo tensor de recuadros (7,7,30)\n",
    "\n",
    "        w_img=int(img.size[0]) # Image width\n",
    "        h_img=int(img.size[1]) # Image height\n",
    "\n",
    "        img2 = transormation(img) #Proceso la imagen con la transformacion\n",
    "        \n",
    "        all_bbox = ann['annotation']['object'] #Obtengo todos los recuadros disponibles\n",
    "        if type(all_bbox) == dict: # inconsistency in the annotation file\n",
    "            all_bbox = [all_bbox]\n",
    "        for bbox_idx, one_bbox in enumerate(all_bbox):\n",
    "            bbox = one_bbox['bndbox'] #Obtengo parametros del recuadro x1 y1 x2 y2\n",
    "            obj_cls = one_bbox['name'] #Obtengo la clase del objeto del rectangulo\n",
    "\n",
    "            x = (int(bbox['xmax']) + int(bbox['xmin']))/(2*w_img) #Busco centro x del recuadro\n",
    "            y = (int(bbox['ymax']) + int(bbox['ymin']))/(2*h_img) #Busco centro y del recuadro\n",
    "            w = (int(bbox['xmax'])-int(bbox['xmin']))/w_img #Calculo ancho del recuadro\n",
    "            h = (int(bbox['ymax'])-int(bbox['ymin']))/h_img #Calculo alto del recuadro\n",
    "            class_label = class_to_id[obj_cls] #Paso a numero la clase del objeto\n",
    "\n",
    "            i, j = int(S * y), int(S * x) #Obtengo pos de esq sup izquierda del cuadrante q contiene el centro\n",
    "            x_cell, y_cell = S * x - j, S * y - i #Obtengo x e y relativo a esq sup izq del cuadrante correspondiente\n",
    "            width_cell = w * S #Obtengo el ancho en relacion al ancho de la celda\n",
    "            height_cell = h * S #Obtengo el alto en relación al alto de la celda\n",
    "\n",
    "            if box_batch[i, j, 20] == 0: #Objeto no presente en ese cuadrante\n",
    "                # Set that there exists an object\n",
    "                box_batch[i, j, 20] = 1\n",
    "                # Box coordinates\n",
    "                box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n",
    "                #box_coordinates = torch.tensor([x, y, w, h])\n",
    "                box_batch[i, j, 21:25] = box_coordinates\n",
    "                # Set one hot encoding for class_label\n",
    "                box_batch[i, j, class_label] = 1 #Pongo 1 en clase correspondiente\n",
    "\n",
    "        return img2, box_batch\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "    def get_trainset(self):\n",
    "        return self.train_data\n",
    "    \n",
    "    def get_testset(self):\n",
    "        return self.test_data\n",
    "    \n",
    "    def get_trainset_clean(self):\n",
    "        return self.train_data_clean\n",
    "    \n",
    "    def get_testset_clean(self):\n",
    "        return self.test_data_clean\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader( self.train_data,batch_size=self.batch_size, pin_memory=True,num_workers=parameter[\"workers\"]) \n",
    "  \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data,batch_size=self.batch_size, pin_memory=True,num_workers=parameter[\"workers\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCordinatesFromBBoxes (BBoxes):\n",
    "    x1 = BBoxes[..., 0:1] - BBoxes[..., 2:3] / 2\n",
    "    y1 = BBoxes[..., 1:2] - BBoxes[..., 3:4] / 2\n",
    "    x2 = BBoxes[..., 0:1] + BBoxes[..., 2:3] / 2\n",
    "    y2 = BBoxes[..., 1:2] + BBoxes[..., 3:4] / 2\n",
    "\n",
    "    return x1, y1 , x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics (boxes_predictions, boxes_labels):\n",
    "    box1_x1, box1_y1, box1_x2, box1_y2= getCordinatesFromBBoxes(boxes_predictions)\n",
    "    box2_x1, box2_y1, box2_x2, box2_y2= getCordinatesFromBBoxes(boxes_labels)\n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # .clamp(0) is for the case when they do not intersect\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    iou = intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "    return iou\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize boundingboxes and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_wiewner(sample, predicted=None, S=7, B=2, C=20):\n",
    "    image = sample[0] \n",
    "    annotation = sample [1]\n",
    "\n",
    "    width,height= image.size\n",
    "    print(width)\n",
    "    print(height)\n",
    "      \n",
    "    all_bbox = annotation['annotation']['object'] #Obtengo todos los recuadros disponibles\n",
    "    if type(all_bbox) == dict: # inconsistency in the annotation file\n",
    "        all_bbox = [all_bbox]\n",
    "    \n",
    "    image1 = ImageDraw.Draw(image)  \n",
    "    for bbox_idx, one_bbox in enumerate(all_bbox):\n",
    "        bbox = one_bbox['bndbox'] #Obtengo parametros del recuadro x1 y1 x2 y2\n",
    "        shape=[(int(bbox[\"xmin\"]),int(bbox[\"ymin\"])),(int(bbox['xmax']),int(bbox[\"ymax\"]))]\n",
    "        image1.rectangle(shape, outline =\"red\")\n",
    "\n",
    "        obj_cls = one_bbox['name'] #Obtengo la clase del objeto del rectangulo\n",
    "        image1.text(shape[0], obj_cls,align =\"left\", fill=\"red\") \n",
    "    \n",
    "    if(predicted is not None):\n",
    "        print(predicted.size())\n",
    "        if (predicted.size() != torch.Size([S, S, C + B * 5])):\n",
    "            predicted = predicted.reshape(S, S, C + B * 5)\n",
    "            print(predicted.size())\n",
    "\n",
    "        for i in range(S):\n",
    "            for j in range(S):\n",
    "\n",
    "                if  predicted[i][j][20]>0.50: #Veo presencia de objeto\n",
    "                    box = predicted[i][j][21:25].tolist()\n",
    "                    obj_class = predicted[i][j][0:19].tolist()\n",
    "\n",
    "                    upper_left_x = float(j*width/S) + float(box[0]*width/S) - float(box[2]*width/(2*S))\n",
    "                    upper_left_y = float(i*height/S) + float(box[1]*height/S) - float(box[3]*height/(2*S))\n",
    "                \n",
    "                    bottom_left_x = float(j*width/S) + float(box[0]*width/S) + float(box[2]*width/(2*S))\n",
    "                    bottom_left_y = float(i*height/S) + float(box[1]*height/S) + float(box[3]*height/(2*S))\n",
    "\n",
    "                    shape= [(int(upper_left_x),int(upper_left_y)), (int(bottom_left_x),int(bottom_left_y))]\n",
    "                    image1 .rectangle(shape, outline =\"green\")\n",
    "                    \n",
    "                    obj_cls = id_to_class[np.argmax(obj_class)]\n",
    "                    image1 .text(shape[0], obj_cls,align =\"left\", fill=\"green\") \n",
    "\n",
    "                if predicted[i][j][25]>0.50: #Veo presencia de objeto\n",
    "                    box = predicted[i][j][26:30].tolist()\n",
    "                    obj_class = predicted[i][j][0:19].tolist()\n",
    "\n",
    "                    upper_left_x = float(j*width/S) + float(box[0]*width/S) - float(box[2]*width/(2*S))\n",
    "                    upper_left_y = float(i*height/S) + float(box[1]*height/S) - float(box[3]*height/(2*S))\n",
    "                \n",
    "                    bottom_left_x = float(j*width/S) + float(box[0]*width/S) + float(box[2]*width/(2*S))\n",
    "                    bottom_left_y = float(i*height/S) + float(box[1]*height/S) + float(box[3]*height/(2*S))\n",
    "\n",
    "                    shape= [(int(upper_left_x),int(upper_left_y)), (int(bottom_left_x),int(bottom_left_y))]\n",
    "                    image1 .rectangle(shape, outline =\"green\")\n",
    "                    \n",
    "                    obj_cls = id_to_class[np.argmax(obj_class)]\n",
    "                    image1 .text(shape[0], obj_cls,align =\"left\", fill=\"green\") \n",
    "  \n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWXKG6LPKKxh"
   },
   "source": [
    "### Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "\n",
    "def ConvMixer(dim, depth, kernel_size, patch_size, n_classes):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(dim),\n",
    "        *[nn.Sequential(\n",
    "                Residual(nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm2d(dim)\n",
    "                )),\n",
    "                nn.Conv2d(dim, dim, kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(dim)\n",
    "        ) for i in range(depth)],\n",
    "         nn.AdaptiveAvgPool2d((1,1)),\n",
    "         nn.Flatten(),\n",
    "         nn.Linear(dim, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from symbol import parameters\n",
    "\n",
    "\n",
    "class ConvMixerModule(pl.LightningModule):\n",
    "    def __init__(self, checkpoint=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = ConvMixer(parameter['hdim'], parameter['depth'], patch_size=parameter['psize'], kernel_size=parameter['conv-ks'], n_classes=1000)\n",
    " \n",
    "        self.criterion= YoloLoss()\n",
    "        self.train_loss=0\n",
    "        self.train_acc=0\n",
    "        save = torch.load(\"convmixer_768_32_ks7_p7_relu.pth.tar\")\n",
    "        self.model.load_state_dict(save)\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-3],\n",
    "                nn.AdaptiveAvgPool2d((8,8)),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(parameter['hdim']*64, 496),\n",
    "                nn.Dropout(0.0),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Linear(496, S * S * (C + B * 5)))\n",
    "\n",
    "    def forward (self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img_batch, box_batch= batch    \n",
    "        predictions = self(img_batch)\n",
    "        loss =self.criterion(predictions,box_batch)\n",
    "\n",
    "        #predictions = predictions.reshape(-1, S, S, C + B * 5)\n",
    "        #print(statistics(predictions[..., 21:25], box_batch[..., 21:25]))\n",
    "\n",
    "        self.log('train_loss', loss,on_step=True, on_epoch=True,prog_bar=True)\n",
    "        return loss\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        img_batch, box_batch= batch    \n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = self(img_batch)\n",
    "            val_loss =self.criterion(predictions,box_batch)\n",
    "        self.log('val_loss', val_loss,on_step=True, on_epoch=True,prog_bar=True)\n",
    "        return val_loss\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        img_batch, box_batch= batch    \n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = self(img_batch)\n",
    "            test_loss =self.criterion(predictions,box_batch)\n",
    "            self.log('test_loss', test_loss,on_step=True, on_epoch=True,prog_bar=True)\n",
    "        return test_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=parameter['lr-max'], betas=(0.9, 0.999))\n",
    "        return optimizer\n",
    "   \n",
    "    def save(self, path= '/models'):\n",
    "        torch.save(self.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | model     | Sequential | 24.9 M\n",
      "1 | criterion | YoloLoss   | 0     \n",
      "-----------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.434    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdd8ab18b264e2cb097cddd5a082882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model  = ConvMixerModule.load_from_checkpoint(\"checkpoints/last.ckpt\")\n",
    "#model  = ConvMixerModule()\n",
    "data = VOC()\n",
    "ddp= DDPStrategy(find_unused_parameters=False)\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss',dirpath='checkpoints',\n",
    "                                         filename='file',save_last=True, every_n_epochs=1, save_top_k=5)\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "trainer = pl.Trainer(max_epochs=parameter[\"epochs\"], auto_lr_find=False, auto_scale_batch_size=False,\n",
    "                  gpus='0,1,2',precision=16,\n",
    "                  callbacks=[checkpoint_callback, lr_monitor], \n",
    "                   strategy= ddp,\n",
    "                  logger=logger\n",
    "                 )\n",
    "#trainer = pl.Trainer(gpus=1, max_epochs=parameter[\"epochs\"], callbacks=[lr_monitor])\n",
    "trainer.fit(model,data )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(max_epochs=parameter[\"epochs\"], auto_lr_find=False, auto_scale_batch_size=False,\n",
    "#                   gpus='0,2',precision=16,\n",
    "#                   callbacks=[checkpoint_callback, lr_monitor], \n",
    "#                    strategy= ddp\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAndVisualize(n):\n",
    "    dataset= VOC()\n",
    "    original= dataset.get_trainset_clean()\n",
    "    processed = dataset.get_trainset()\n",
    "    tensor= processed[n][0]\n",
    "\n",
    "    tensor= tensor.reshape(1, *tensor.size())\n",
    "    tensor.to(torch.device(\"cuda\"))\n",
    "\n",
    "    label = model(tensor)\n",
    "    label=label.reshape(1470)\n",
    "\n",
    "    voc_wiewner(original[n],label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testAndVisualize(150)\n",
    "#testAndVisualize(250)\n",
    "#testAndVisualize(350)\n",
    "\n",
    "#testAndVisualize(450)\n",
    "testAndVisualize(570)\n",
    "#testAndVisualize(650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "YOLOvJuan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
